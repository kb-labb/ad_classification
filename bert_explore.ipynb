{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitpytorch3kcondadaa5fe21d40f4b6ebd839bacb5a75965",
   "display_name": "Python 3.8.5 64-bit ('pytorch3k': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"image_path\": [os.getcwd() + '/images/' + image_path for image_path in os.listdir('images')]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text.txt\") as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "content = [x.strip(\"\\n\") for x in content]\n",
    "content = list(filter(len, content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:106]\n",
    "df[\"text\"] = list(filter(None, pd.Series(content).str.split(\".\").explode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = np.random.randint(low=0, high=2, size=106)\n",
    "df[\"label\"] = np.repeat([1, 0], [90, 16])\n",
    "df[\"label\"] = 0\n",
    "# df[\"label\"][df[\"text\"].apply(len) > 90] = 1\n",
    "df[\"label\"][df[\"text\"].str.contains(\"filosofi\")] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=(256, 256), \n",
    "                      interpolation=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvertisementDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset with cropped images and text from OCR performed on newspapers.\"\"\"\n",
    "\n",
    "    def __init__(self, df, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (DataFrame): DataFrame with text, image path and label annotations.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.tokenizer = torch.hub.load('huggingface/pytorch-transformers', \n",
    "                                        'tokenizer', \n",
    "                                        'KB/bert-base-swedish-cased')  # Download vocabulary from S3 and cache.\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def load_image(self, index):\n",
    "        image_path = self.df.iloc[index].image_path\n",
    "        image = Image.open(image_path)\n",
    "        return(image)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        df_row = self.df.iloc[index]\n",
    "        label = df_row[\"label\"]\n",
    "\n",
    "        # Text\n",
    "        ocr_text = df_row[\"text\"]\n",
    "        token_info = self.tokenizer.encode_plus(ocr_text, \n",
    "                                                max_length=64, \n",
    "                                                truncation=True, \n",
    "                                                pad_to_max_length=True,\n",
    "                                                return_tensors=\"pt\")\n",
    "\n",
    "        # token_output = {\"input_ids\": token_info[\"input_ids\"].to(\"cuda\"),\n",
    "        #                 \"token_type_ids\": token_info[\"token_type_ids\"].to(\"cuda\"),\n",
    "        #                 \"attention_mask\": token_info[\"attention_mask\"].to(\"cuda\")}\n",
    "\n",
    "        # Image\n",
    "        image_path = df_row[\"image_path\"]\n",
    "        image = self.load_image(index)\n",
    "        if self.transform:\n",
    "            image = transform(image) # Resize to 256x256 and imagenet normalization\n",
    "\n",
    "        return image, token_info, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/faton/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
     ]
    }
   ],
   "source": [
    "dataset = AdvertisementDataset(df=df, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a = iter(dataloader)\n",
    "# next(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertResnetClassifier(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__() # Initialize superclass nn.Module\n",
    "        if pretrained:\n",
    "            self.bert = torch.hub.load('huggingface/pytorch-transformers', 'model', 'KB/bert-base-swedish-cased', output_hidden_states=True)\n",
    "            self.resnet50 = models.resnet50(pretrained=pretrained)\n",
    "            self.resnet50.fc = nn.Identity() # Remove fc layer (classification head)\n",
    "        else:\n",
    "            # Load saved models from disk\n",
    "            pass\n",
    "        self.linear1 = nn.Linear(2816, 512)\n",
    "        self.linear2 = nn.Linear(512, 1) # 1 output class.\n",
    "\n",
    "    def forward(self, image, token_ids, type_ids, mask):\n",
    "        # image.unsqueeze_(0) # Add batch dimension to image tensor\n",
    "        image_embedding = self.resnet50(image)\n",
    "        hidden_states = self.bert(token_ids, token_type_ids=type_ids, attention_mask=mask)\n",
    "        \n",
    "        output_embedding = torch.cat([image_embedding, hidden_states[0][:,0,:]], dim=1) # (1, 2048) (image) + (1, 768) (text) = (1, 2816)\n",
    "        linear1_output = self.linear1(output_embedding)\n",
    "        linear2_output = self.linear2(linear1_output)\n",
    "\n",
    "        return linear2_output\n",
    "\n",
    "    def __str__(self):\n",
    "        print(self.temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/faton/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
     ]
    }
   ],
   "source": [
    "model = BertResnetClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'GeForce RTX 3090'"
      ]
     },
     "metadata": {},
     "execution_count": 446
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = model(image=dataset[0][0],\n",
    "#       token_ids=dataset[1][1][\"input_ids\"],\n",
    "#       type_ids=dataset[1][1][\"token_type_ids\"],\n",
    "#       mask=dataset[1][1][\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, batch in enumerate(dataloader):\n",
    "    images = batch[0]\n",
    "    texts = batch[1]\n",
    "    labels = batch[2].to(\"cuda\")\n",
    "    optimizer.zero_grad()\n",
    "    output = model(image=images.to(\"cuda\"),\n",
    "                   token_ids=texts[\"input_ids\"].squeeze(dim=1).to(\"cuda\"),\n",
    "                   type_ids=texts[\"token_type_ids\"].squeeze(dim=1).to(\"cuda\"),\n",
    "                   mask=texts[\"attention_mask\"].squeeze(dim=1).to(\"cuda\"))\n",
    "\n",
    "    labels = labels.unsqueeze(1).type_as(output) # (8) -> (8, 1) and long to float\n",
    "    loss = loss_fn(output, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                          image_path  \\\n",
       "0  /home/faton/projects/ad_classification/images/...   \n",
       "1  /home/faton/projects/ad_classification/images/...   \n",
       "2  /home/faton/projects/ad_classification/images/...   \n",
       "3  /home/faton/projects/ad_classification/images/...   \n",
       "4  /home/faton/projects/ad_classification/images/...   \n",
       "5  /home/faton/projects/ad_classification/images/...   \n",
       "6  /home/faton/projects/ad_classification/images/...   \n",
       "7  /home/faton/projects/ad_classification/images/...   \n",
       "\n",
       "                                                text  label         probs  \n",
       "0  Simone de Beauvoir måste räknas som en av de m...      0  1.757294e-03  \n",
       "1   Senast gjorde hon sig påmind genom ett Goncou...      0  6.978888e-03  \n",
       "2   Men Goncourtakademien krönte denna gång en re...      1  2.129315e-01  \n",
       "3   Det mest berömda liksom det mest omfattande a...      0  9.290784e-05  \n",
       "4  Det är en undersökning på mycket bred bas som ...      0  1.352533e-04  \n",
       "5   Historia, filosofi och etnologi har fått släp...      1  1.430624e-01  \n",
       "6   I förbigående bör nämnas att Simone de Beauvo...      0  4.863443e-08  \n",
       "7   (Det är säkert ingen händelse att den kvinnli...      0  2.598913e-07  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>text</th>\n      <th>label</th>\n      <th>probs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/home/faton/projects/ad_classification/images/...</td>\n      <td>Simone de Beauvoir måste räknas som en av de m...</td>\n      <td>0</td>\n      <td>1.757294e-03</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/home/faton/projects/ad_classification/images/...</td>\n      <td>Senast gjorde hon sig påmind genom ett Goncou...</td>\n      <td>0</td>\n      <td>6.978888e-03</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/home/faton/projects/ad_classification/images/...</td>\n      <td>Men Goncourtakademien krönte denna gång en re...</td>\n      <td>1</td>\n      <td>2.129315e-01</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/home/faton/projects/ad_classification/images/...</td>\n      <td>Det mest berömda liksom det mest omfattande a...</td>\n      <td>0</td>\n      <td>9.290784e-05</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/home/faton/projects/ad_classification/images/...</td>\n      <td>Det är en undersökning på mycket bred bas som ...</td>\n      <td>0</td>\n      <td>1.352533e-04</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>/home/faton/projects/ad_classification/images/...</td>\n      <td>Historia, filosofi och etnologi har fått släp...</td>\n      <td>1</td>\n      <td>1.430624e-01</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>/home/faton/projects/ad_classification/images/...</td>\n      <td>I förbigående bör nämnas att Simone de Beauvo...</td>\n      <td>0</td>\n      <td>4.863443e-08</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>/home/faton/projects/ad_classification/images/...</td>\n      <td>(Det är säkert ingen händelse att den kvinnli...</td>\n      <td>0</td>\n      <td>2.598913e-07</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 507
    }
   ],
   "source": [
    "df[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[3.5975e-08],\n",
      "        [7.1089e-04],\n",
      "        [4.5710e-01],\n",
      "        [2.5725e-05],\n",
      "        [2.1728e-08],\n",
      "        [3.9464e-01],\n",
      "        [5.7320e-07],\n",
      "        [1.9266e-06]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0')\n",
      "tensor([[1.2702e-10],\n",
      "        [3.3323e-06],\n",
      "        [2.2379e-05],\n",
      "        [4.8090e-06],\n",
      "        [4.4815e-03],\n",
      "        [6.9364e-04],\n",
      "        [1.4362e-05],\n",
      "        [3.4696e-04]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0')\n",
      "tensor([[1.1649e-04],\n",
      "        [1.9955e-05],\n",
      "        [2.4999e-05],\n",
      "        [1.0151e-04],\n",
      "        [3.3446e-07],\n",
      "        [8.1839e-02],\n",
      "        [7.2932e-09],\n",
      "        [1.1029e-04]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0')\n",
      "tensor([[1.2022e-05],\n",
      "        [5.7819e-05],\n",
      "        [1.1871e-08],\n",
      "        [6.0089e-05],\n",
      "        [2.8478e-03],\n",
      "        [1.0024e-03],\n",
      "        [2.2246e-03],\n",
      "        [1.3802e-06]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0')\n",
      "tensor([[5.9580e-11],\n",
      "        [1.3163e-04],\n",
      "        [7.6017e-05],\n",
      "        [2.9018e-08],\n",
      "        [1.3850e-05],\n",
      "        [2.2673e-02],\n",
      "        [1.3521e-04],\n",
      "        [7.3872e-04]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0')\n",
      "tensor([[1.2385e-09],\n",
      "        [8.1668e-04],\n",
      "        [1.0840e-03],\n",
      "        [3.2292e-06],\n",
      "        [3.9760e-03],\n",
      "        [8.5488e-05],\n",
      "        [1.1736e-06],\n",
      "        [5.0383e-04]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0')\n",
      "tensor([[6.3606e-04],\n",
      "        [3.5918e-04],\n",
      "        [5.1573e-05],\n",
      "        [2.0125e-07],\n",
      "        [1.1676e-08],\n",
      "        [2.6794e-05],\n",
      "        [1.4880e-03],\n",
      "        [4.7158e-04]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0')\n",
      "tensor([[2.1788e-03],\n",
      "        [1.2774e-05],\n",
      "        [3.6412e-04],\n",
      "        [2.6167e-03],\n",
      "        [9.8150e-07],\n",
      "        [4.6086e-04],\n",
      "        [7.0304e-10],\n",
      "        [9.9635e-05]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0')\n",
      "tensor([[2.2357e-03],\n",
      "        [4.5083e-05],\n",
      "        [8.6629e-07],\n",
      "        [1.2618e-04],\n",
      "        [2.0778e-05],\n",
      "        [4.0533e-07],\n",
      "        [2.1959e-05],\n",
      "        [1.7144e-09]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0')\n",
      "tensor([[6.0253e-07],\n",
      "        [6.6506e-06],\n",
      "        [9.0015e-07],\n",
      "        [1.1876e-04],\n",
      "        [1.2552e-04],\n",
      "        [1.3536e-08],\n",
      "        [1.6209e-07],\n",
      "        [1.3993e-03]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0')\n",
      "tensor([[1.2926e-03],\n",
      "        [1.1468e-03],\n",
      "        [1.4925e-04],\n",
      "        [1.9127e-06],\n",
      "        [3.7110e-07],\n",
      "        [7.2278e-07],\n",
      "        [1.1859e-05],\n",
      "        [1.3353e-04]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0')\n",
      "tensor([[8.8970e-05],\n",
      "        [6.7158e-08],\n",
      "        [1.3594e-04],\n",
      "        [8.7006e-05],\n",
      "        [1.1583e-03],\n",
      "        [2.5420e-07],\n",
      "        [8.5693e-07],\n",
      "        [6.4962e-05]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0')\n",
      "tensor([[7.9745e-06],\n",
      "        [9.6914e-08],\n",
      "        [2.8738e-04],\n",
      "        [1.6848e-04],\n",
      "        [1.2477e-04],\n",
      "        [2.0145e-07],\n",
      "        [1.0283e-03],\n",
      "        [1.5730e-06]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0')\n",
      "tensor([[4.6110e-09],\n",
      "        [2.0470e-03]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.],\n",
      "        [0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "probs_list = []\n",
    "\n",
    "for i, batch in enumerate(testloader):\n",
    "    images = batch[0]\n",
    "    texts = batch[1]\n",
    "    labels = batch[2].to(\"cuda\")\n",
    "    output = model(image=images.to(\"cuda\"),\n",
    "                   token_ids=texts[\"input_ids\"].squeeze(dim=1).to(\"cuda\"),\n",
    "                   type_ids=texts[\"token_type_ids\"].squeeze(dim=1).to(\"cuda\"),\n",
    "                   mask=texts[\"attention_mask\"].squeeze(dim=1).to(\"cuda\"))\n",
    "\n",
    "    probs_list += torch.nn.functional.sigmoid(output).tolist()\n",
    "    print(torch.nn.functional.sigmoid(output))\n",
    "    print((torch.nn.functional.sigmoid(output) > 0.5).float()) # predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"probs\"] = sum(probs_list, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           image_path  \\\n",
       "2   /home/faton/projects/ad_classification/images/...   \n",
       "5   /home/faton/projects/ad_classification/images/...   \n",
       "21  /home/faton/projects/ad_classification/images/...   \n",
       "37  /home/faton/projects/ad_classification/images/...   \n",
       "12  /home/faton/projects/ad_classification/images/...   \n",
       "..                                                ...   \n",
       "71  /home/faton/projects/ad_classification/images/...   \n",
       "40  /home/faton/projects/ad_classification/images/...   \n",
       "62  /home/faton/projects/ad_classification/images/...   \n",
       "8   /home/faton/projects/ad_classification/images/...   \n",
       "32  /home/faton/projects/ad_classification/images/...   \n",
       "\n",
       "                                                 text  label         probs  \n",
       "2    Men Goncourtakademien krönte denna gång en re...      1  4.570983e-01  \n",
       "5    Historia, filosofi och etnologi har fått släp...      1  3.946420e-01  \n",
       "21   I och med att kvinnan såsom hemgifts- eller a...      0  8.183917e-02  \n",
       "37   Själva framtidsperspektivet ter sig, i synner...      0  2.267263e-02  \n",
       "12   Sedan behövs det bara ett enkelt trick för at...      0  4.481472e-03  \n",
       "..                                                ...    ...           ...  \n",
       "71  Å andra sidan är det säkert att mannens psykof...      0  1.714375e-09  \n",
       "40   Förändringen är i varje fall inte av tillräck...      0  1.238455e-09  \n",
       "62   Den \"kvinnliga uppfostran” syftar, eller har ...      0  7.030374e-10  \n",
       "8   Är kvinnan mannen underlägsen och i så fall va...      0  1.270227e-10  \n",
       "32             Uppfostran komplicerar också problemet      0  5.957988e-11  \n",
       "\n",
       "[106 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>text</th>\n      <th>label</th>\n      <th>probs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>/home/faton/projects/ad_classification/images/...</td>\n      <td>Men Goncourtakademien krönte denna gång en re...</td>\n      <td>1</td>\n      <td>4.570983e-01</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>/home/faton/projects/ad_classification/images/...</td>\n      <td>Historia, filosofi och etnologi har fått släp...</td>\n      <td>1</td>\n      <td>3.946420e-01</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>/home/faton/projects/ad_classification/images/...</td>\n      <td>I och med att kvinnan såsom hemgifts- eller a...</td>\n      <td>0</td>\n      <td>8.183917e-02</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>/home/faton/projects/ad_classification/images/...</td>\n      <td>Själva framtidsperspektivet ter sig, i synner...</td>\n      <td>0</td>\n      <td>2.267263e-02</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>/home/faton/projects/ad_classification/images/...</td>\n      <td>Sedan behövs det bara ett enkelt trick för at...</td>\n      <td>0</td>\n      <td>4.481472e-03</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>/home/faton/projects/ad_classification/images/...</td>\n      <td>Å andra sidan är det säkert att mannens psykof...</td>\n      <td>0</td>\n      <td>1.714375e-09</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>/home/faton/projects/ad_classification/images/...</td>\n      <td>Förändringen är i varje fall inte av tillräck...</td>\n      <td>0</td>\n      <td>1.238455e-09</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>/home/faton/projects/ad_classification/images/...</td>\n      <td>Den \"kvinnliga uppfostran” syftar, eller har ...</td>\n      <td>0</td>\n      <td>7.030374e-10</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>/home/faton/projects/ad_classification/images/...</td>\n      <td>Är kvinnan mannen underlägsen och i så fall va...</td>\n      <td>0</td>\n      <td>1.270227e-10</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>/home/faton/projects/ad_classification/images/...</td>\n      <td>Uppfostran komplicerar också problemet</td>\n      <td>0</td>\n      <td>5.957988e-11</td>\n    </tr>\n  </tbody>\n</table>\n<p>106 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 531
    }
   ],
   "source": [
    "df.sort_values(by = \"probs\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "dataset[0][0].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"Vem var Jim Hansson?\"\n",
    "text_2 = \"Jim Hansson var en skådespelare\"\n",
    "text_3 = \"Det var jag.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_info = tokenizer.encode_plus(text_1, max_length=32, truncation=True, pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encoded_layers = model(torch.tensor([token_info[\"input_ids\"]]), \n",
    "                            token_type_ids=torch.tensor([token_info[\"token_type_ids\"]]), \n",
    "                            attention_mask=torch.tensor([token_info[\"attention_mask\"]])) # last_hidden_state, pooled output (CLS token through some activation), hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The model has 201 named parameters.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The model has {len(params)} named parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "bert.embeddings.word_embeddings.weight                  (50325, 768)\nbert.embeddings.position_embeddings.weight              (512, 768)\nbert.embeddings.token_type_embeddings.weight            (2, 768)\nbert.embeddings.LayerNorm.weight                        (768,)\nbert.embeddings.LayerNorm.bias                          (768,)\nbert.encoder.layer.0.attention.self.query.weight        (768, 768)\nbert.encoder.layer.0.attention.self.query.bias          (768,)\nbert.encoder.layer.0.attention.self.key.weight          (768, 768)\nbert.encoder.layer.0.attention.self.key.bias            (768,)\nbert.encoder.layer.0.attention.self.value.weight        (768, 768)\nbert.encoder.layer.0.attention.self.value.bias          (768,)\nbert.encoder.layer.0.attention.output.dense.weight      (768, 768)\nbert.encoder.layer.0.attention.output.dense.bias        (768,)\nbert.encoder.layer.0.attention.output.LayerNorm.weight  (768,)\nbert.encoder.layer.0.attention.output.LayerNorm.bias    (768,)\nbert.encoder.layer.0.intermediate.dense.weight          (3072, 768)\nbert.encoder.layer.0.intermediate.dense.bias            (3072,)\nbert.encoder.layer.0.output.dense.weight                (768, 3072)\nbert.encoder.layer.0.output.dense.bias                  (768,)\nbert.encoder.layer.0.output.LayerNorm.weight            (768,)\nbert.encoder.layer.0.output.LayerNorm.bias              (768,)\nbert.pooler.dense.weight                                (768, 768)\nbert.pooler.dense.bias                                  (768,)\nclassifier.weight                                       (5, 768)\nclassifier.bias                                         (5,)\n"
     ]
    }
   ],
   "source": [
    "for p in params[0:5]:\n",
    "    print(f\"{p[0]:55} {tuple(p[1].size())}\")\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(f\"{p[0]:55} {tuple(p[1].size())}\")\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(f\"{p[0]:55} {tuple(p[1].size())}\")"
   ]
  }
 ]
}